{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Name: Zhining Zhou\n",
    "\n",
    "Andrew ID: zhiningz\n",
    "\n",
    "Collaborators (if none, say \"none\"; do *not* leave this blank): none\n",
    "\n",
    "Reminder: you should not be sharing code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions\n",
    "1. Fill in your name, Andrew ID, and collaborators above.\n",
    "2. Fill in the code/text blocks to answer each question.\n",
    "3. Do *not* change any of the existing code provided.\n",
    "4. Run the entire notebook *before* submitting it on Canvas to make sure that the code actually runs without errors. (**Important**: Any code cells that you have entered code for but did not actually execute will be disregarded, so please be sure to actually run your code first and make sure it runs without errors! We may re-run a subset of your code for grading purposes.)\n",
    "5. Be careful about where you save data for use with this Jupyter notebook (more details on this later)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Problem 1] The Distribution of Words in a Human Language: Zipf's Law [45 pts]\n",
    "\n",
    "In this problem, you will be looking at a phenomenon of \"natural languages\" (i.e., human languages, such as English). As it turns out, when we look at the distribution of words in a different human languages, the distribution roughly follows what's called *Zipf's law* ([Wikipedia](https://en.wikipedia.org/wiki/Zipf%27s_law)).\n",
    "\n",
    "*Zipf's law* states that given a large sample of words used, any word's frequency is inversely proportional to its rank in the frequency table (the word with rank 1 is the most frequently occurring word, the word with rank 2 is the second most frequently occurring word, etc). In this problem, you are going to test Zipf's law on a real dataset and explore some of its properties.\n",
    "\n",
    "This problem assumes you have already installed the `spaCy` package ([https://spacy.io](https://spacy.io)). Instructions for installing this are part of the tutorial file \"`Anaconda, Jupyter, and spaCy setup tutorial.pdf`\". You can find how to access lemmatized tokens using `spaCy` by looking at the `spaCy` demo Jupyter notebook from lecture.\n",
    "\n",
    "Note: For this problem, you don't actually need `spaCy`'s named entity recognition, grammatical parsing, or part-of-speech tagging. Turning these elements off when you instantiate the nlp object can substantially speed up your code.  To make sure these are off when instantiating the nlp object, call: \n",
    "\n",
    "`\n",
    "nlp = spacy.load('en', disable=['ner', 'parser', 'tagger'])\n",
    "`\n",
    "\n",
    "\n",
    "(a) [5 pts] Read the data files.\n",
    "\n",
    "The dataset is a collection of the 100 most popular books downloaded from Gutenburg Project ([https://www.gutenberg.org/browse/scores/top](https://www.gutenberg.org/browse/scores/top)). Each file contains the text of a book. Now read all the texts from disk. This will be the corpus in this problem.\n",
    "\n",
    "Note: Please DO NOT change the folder name or the path, and make sure you use a relative path (e.g. './HW1_Dataset/*filename*) when reading the files. When grading your homework, we will put your Jupyter notebook file and the dataset in the same folder, and run your code. **You will receive 0 points for this problem if your code fails to load the data.**\n",
    "\n",
    "Hint: To list all files that match a certain pattern, you can use the `glob` package. Here's an example usage:\n",
    "\n",
    "`\n",
    "import glob\n",
    "print(glob.glob('./HW1_Dataset/*.txt'))\n",
    "`\n",
    "\n",
    "Hint: When debugging your code, you may want to first make sure your code runs on a few of the books rather than all 100 (for example, you can start by only having 3 of the text files in `HW1_Dataset`). Once you're confident that your solution is correct on a few text files, then run on all of them! This is a standard approach to debugging code that is meant to handle large datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "########################################################################\n",
    "######################### Write your code here #########################\n",
    "########################################################################\n",
    "import glob\n",
    "book_list = glob.glob('./HW1_Dataset/*.txt')\n",
    "book_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) [15 pts] Build the frequency table. Specifically, complete the following three tasks:\n",
    "\n",
    "  1. Process the text by separating and lemmatizing the words.\n",
    "  2. Then count the number of times each word appears in the corpus and build a frequency table. (In particular, use raw counts as the \"frequency\"--do not divide by the total number of words in the corpus.)\n",
    "  3. Sort the table and print the top 50 most frequent words, along with their frequencies and ranks. Don't worry about ties (for example, if multiple things have the same frequency, it's fine if your solution breaks ties arbitrarily in the sorting).\n",
    "\n",
    "Note: When counting the words, only include words (tokens) that consist of alphabetic letters (a-z and A-Z). You can do this with what's called a *regular expression*. For example, to check whether the words \"will.i.am\" or \"Tesla\" are alphabetic, you would do the following:\n",
    "\n",
    "`\n",
    "import re  # regular expression package\n",
    "if re.match('[a-zA-Z]+$', 'will.i.am'):\n",
    "    print('will.i.am consists only of alphabetic letters!')\n",
    "if re.match('[a-zA-Z]+$', 'Tesla'):\n",
    "    print('tesla consists only of alphabetic letters!')\n",
    "`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################\n",
    "######################### Write your code here #########################\n",
    "########################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c) [10 pts] Visualize the frequency table by plotting a **raw scatter plot** (put frequency as the y-axis and rank as the x-axis), and a **log-log plot** (use logarithmic scales on both the x- and y- axes). Note that this should be for all words and not only the top 50. As before, for the ranks, do not worry about ties, i.e., break ties arbitrarily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Raw scatter plot\n",
    "########################################################################\n",
    "######################### Write your code here #########################\n",
    "########################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Log-log plot\n",
    "########################################################################\n",
    "######################### Write your code here #########################\n",
    "########################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(d) [15 pts] Explore the frequency table: Zipf's law states that term frequency is governed by a power low, i.e. the relationship between term frequency and rank can be approximated by $f(r) = cr^{-1}$, where $f(r)$ is the frequency of the term at rank $r$, $r$ is the rank of a term, and $c$ is a constant and is approximately 0.1*(corpus size) for English. \n",
    "\n",
    "Answer following questions:\n",
    "\n",
    "- What do you observe in the log-log plot above? Is this consist with the power law?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your text answer (for this question, your answer is *not* code): *** WRITE YOUR ANSWER HERE ***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Think of the corpus as a (large) unigram bag of words. Following the analogy from lecture, imagine drawing a single word from this big bag (note that we are assuming that we've lemmatized the words and also filtered out non-alphanumeric words). What is the probability of drawing one of the 4 most frequent words? What is the probability of drawing one of the 50 most frequent words? Answer these two questions using code rather than just entering in the final answers as numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Probability of drawing one of the 4 most frequent words: ')\n",
    "########################################################################\n",
    "######################### Write your code here #########################\n",
    "########################################################################\n",
    "print('Probability of drawing one of the 50 most frequent words: ')\n",
    "########################################################################\n",
    "######################### Write your code here #########################\n",
    "########################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- What proportion of the words occur only once? What proportion of the words occur fewer than 10 times?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Occur only once: \")\n",
    "########################################################################\n",
    "######################### Write your code here #########################\n",
    "########################################################################\n",
    "print(\"Occur fewer than 10 times: \")\n",
    "########################################################################\n",
    "######################### Write your code here #########################\n",
    "########################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Problem 2] Entity Recognition and Pointwise Mutual Information (PMI) [50 pts]\n",
    "By using the entity recognition system in `spaCy`, let's identify named entities from newspaper articles. You'll be using Reuters corpus which contains more than ten thousand newspaper articles. To run the code below, you need to download the Reuters dataset. To do so, in a terminal/command line (recall that you can open a terminal from Jupyter's webpage that shows all the files, which by default is [http://localhost:8888/tree](http://localhost:8888/tree)), start up Python and enter:\n",
    "\n",
    "`\n",
    "import nltk\n",
    "nltk.download('reuters')\n",
    "`\n",
    "\n",
    "Then proceed to the problem subparts below.\n",
    "\n",
    "Note that in this problem you will need named entity recognition but not grammatical parsing or the part-of-speech tagging. Hence, you will want to instantiate the nlp object by calling:\n",
    "\n",
    "`\n",
    "nlp = spacy.load('en', disable=['parser', 'tagger'])\n",
    "`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) [15 pts] Draw a bar chart in which one of the axes shows entity labels and the other shows the frequency of the corresponding label. Use the variables `reuters_nlp` and `label_counter` provided in the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from nltk.corpus import reuters\n",
    "import re\n",
    "import spacy\n",
    "nlp = spacy.load('en', disable=['parser', 'tagger'])\n",
    "reuters_fileids = reuters.fileids()  # hint: when first debugging, consider looking at just the first few\n",
    "reuters_nlp = [nlp(re.sub('\\s+',' ', reuters.raw(i)).strip()) for i in reuters_fileids]\n",
    "label_counter = Counter()\n",
    "########################################################################\n",
    "######################### Write your code here #########################\n",
    "########################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) [15 pts] Now list the top 10 most frequently occured entities (entity text and the number of occurence) with labels `ORG` (organization) and `GPE` (countries, cities, states) respectively.\n",
    "\n",
    "Hint: Here, when counting the frequency, we need to count how many articles have an entity with the desired property. For every article, we add 1 if the article has the entity and 0 otherwise.  List filtering (as in Recitation 1) will be helpful here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################\n",
    "######################### Write your code here #########################\n",
    "########################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c) [20 pts] Give the top 50 `GPE` (countries, cities, states) entities that have the highest Pointwise Mutual Information (PMI) values with regard to the `ORG` (organization) entity **'opec'** (your list of this top 50 should be ranked in decreasing PMI value). Did you find any unexpected results? If so, why do you think it happened? If you found some of the results to be unsurprisingly, how come? \n",
    "\n",
    "Hint 1: As in lecture, when computing PMI, we will compute probabilities by counting the number of documents where entities occur or co-occur.  For example, $P('opec') = \\frac{number \\ \\ of \\ \\ documents \\ \\ containing \\ \\ 'opec'}{number \\ \\ of \\ \\ documents}$.  \n",
    "\n",
    "Hint 2: To compute this ranking, you do not have to compute the full PMI equation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################\n",
    "######################### Write your code here #########################\n",
    "########################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your text answer (for this question, your answer is *not* code): *** WRITE YOUR ANSWER HERE ***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Problem 3] Questionnaire [5 pts]\n",
    "Please provide feedback! To receive the 5 points, be sure to enter your Andrew ID correctly in this Google form: https://goo.gl/forms/tGmCD8L9uI8zNbRG3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
