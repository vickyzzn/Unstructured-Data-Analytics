{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem: Visualizing and Summarizing Email Content\n",
    "\n",
    "In this problem, we will use a variety of tools to visualize and identify similarities in emails.  We will consider textual data from emails sent from employees at Enron. Enron was the sixth largest energy company in the world, before they collasped and the majority of their executives were tried for fraud after overstating the company's earnings by several hundred million dollars.  This dataset is often used by researchers who are interested in \"improving current email tools, or understanding how email is currently used\" because \"it is the only substantial collection of \"real\" email that is public\" (https://www.cs.cmu.edu/~enron/). \n",
    "\n",
    " #### Data Description:\n",
    "The format of the enron_sample.txt file is: \n",
    "\n",
    "---\n",
    "docID wordID count  \n",
    "docID wordID count   \n",
    "...  \n",
    "docID wordID count   \n",
    "docID wordID count  \n",
    "\n",
    "---\n",
    "\n",
    "There are 1000 documents (emails) in this sample.  Individual document names (i.e. a identifier for each docID) are not provided for copyright reasons. \n",
    "\n",
    "The format of the enron_sample_vocab.txt file is wordID = n.  That is, if \"apple\" is the first word in the vocab file, then the wordID for \"apple\" is 0.  \n",
    "\n",
    "We have done much of the necessary pre-processing of this data to save you time.  Please run the code below to load the data.  The enron_sample.txt and enron_sample_vocab.txt files should be in the same folder as your notebook.  The pre-processing code below should take less than 30 seconds to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 9.18 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#Import and pre-process the data\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "#Read in the sparse matrix representation of the document-word count matrix\n",
    "enron_file = open(\"enron_sample.txt\", \"r\") \n",
    "enron_file = enron_file.readlines()\n",
    "\n",
    "all_doc_ids = []\n",
    "all_word_ids = []\n",
    "all_counts = []\n",
    "for l in range(0,len(enron_file)):\n",
    "    l_text = enron_file[l].rstrip()\n",
    "    [l_doc, l_word, l_count] = l_text.split(\",\")\n",
    "    l_doc = int(l_doc) \n",
    "    l_word = int(l_word) \n",
    "    all_doc_ids.append(l_doc)\n",
    "    all_word_ids.append(l_word)\n",
    "    all_counts.append(int(l_count))\n",
    "row = np.array(all_doc_ids)\n",
    "col = np.array(all_word_ids)\n",
    "counts = np.array(all_counts)  \n",
    "\n",
    "num_documents = len(set(all_doc_ids))\n",
    "num_words = len(set(all_word_ids))\n",
    "\n",
    "#Convert to a dense document-word count matrix where the (ith, jth) entry gives the count of the jth word in the ith document\n",
    "import scipy.sparse\n",
    "X = scipy.sparse.csc_matrix((counts, (row,col)),shape=(num_documents,num_words))\n",
    "X_dense = X.todense()\n",
    "\n",
    "#Read in the word names  \n",
    "word_names = open(\"enron_vocab_sample.txt\", \"r\")\n",
    "word_names = word_names.readlines()\n",
    "word_names = [word_names[x] for x in range(0,num_words)]\n",
    "word_names = [x.rstrip() for x in word_names]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part A \n",
    "Please report the 10 most common words in the corpus.  To do this, count the number of times\n",
    "each word appears in the corpus and build a frequency table. \n",
    "(In particular, use counts as the \"frequency\"--do not divide by the total number of words in the corpus.)\n",
    "Sort the table and print the top 10 most frequent words, along with their frequencies and ranks.  \n",
    "\n",
    "Hint: You do not need to load nlp for this question.  Our code for this question ran in less than 10 seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "#Please write your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part B\n",
    "Now, use the `TfidfTransformer` class in `scikit-learn` to transform this data.  Then, use the `LatentDirichletAllocation` class in `scikit-learn` to fit 10 topics to the transformed sample data.  Use 10 max iterations, set `n_jobs=-1` to use all cores on your machine (if it helps). Please set the random_state to 95865 when running LDA. Print the top 5 words associated with each topic.  \n",
    "\n",
    "Our code ran in less than 2 minutes for this section.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "#Take random sample of the data\n",
    "\n",
    "\n",
    "#Create tf-idf matrix\n",
    "\n",
    "\n",
    "#Learn LDA model\n",
    "\n",
    "\n",
    "#Print top words associated with each topic\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part C \n",
    "Please write a few sentences to interpret the results of your topic modeling.  What do the various clusters seem to represent? \n",
    "Do they appear to be meaningful email categories for an energy company?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Here, please write your interpretation:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part D \n",
    "Please use t-SNE and the probability distribution of documents over topics to find a 2-D representation of the email documents. \n",
    "That is, please run t-SNE on the documents' distributions over topics, which is an output of your LDA model. When running t-SNE, set the angle to 0.5, the learning rate to 800, and init to 'PCA'.\n",
    "\n",
    "Our code ran in less than 30 seconds for this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "#Find 2D represenation of the email documents\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part E \n",
    "Use DP means to pick the number of clusters in your 2-D representation, assuming that you are only interested in clusters with probability of at least .05. Set the number of mixture components to 20, the weight concentration prior to .1, n_init to 200, and the random state to 95865.  Please print the sorted probability weights you find.\n",
    "\n",
    "Our code for this section ran in less than 2 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "#Determine the number of clusters using DP-Means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Here, please write your choice for the number of clusters given your DP-Means results:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part F \n",
    "Fit a GMM model with the number of clusters found in part E and visualize the clustering results in a scatter plot. Each document should be represented by its 2-D t-SNE representation and should be color coded according to it's clustering assignment. When running your GMM, set n_init to 200, and the random_state to 95865.\n",
    "\n",
    "Our code for this section ran in less than 10 seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "#Fitting GMM with Number of Clusters Determined By DP-Means\n",
    "\n",
    "#Plot the clustering results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part G\n",
    "Now use sklearn's built-in CH-index function to pick the number of clusters.  To do this you should load metrics from `sklearn` and then use the `calinski_harabaz_score` function.  For more details on this function see: http://scikit-learn.org/stable/modules/generated/sklearn.metrics.calinski_harabaz_score.html \n",
    "\n",
    "You should use a for loop to run k-means for values of k = [2, 5, 10, 20, 30] over your 2-D t-SNE representation of the data.  When running k-means, set n_init to 1000 and random_state to 95865. Please record the CH-index for each k, and plot the results.  Indicate if you would select k to be 2, 5, 10, 20 or 30 given these results. \n",
    "\n",
    "Our code for this section ran in under 1 minute 15 seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "#Determine Number of Clusters with k-means and CH-Index\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Here, please write your choice for the number of clusters based on your analysis using k-means and the CH-Index:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part H\n",
    "Fit k-means with the number of clusters found in part E and visualize the clustering results in a scatter plot. \n",
    "Each document should be represented by its 2-D t-SNE representation and should be color coded according to it's clustering assignment.\n",
    "\n",
    "Then, write a few sentences explaining your results from part G and from this clustering visualization.  Are the clusters found by k-means good? Why might you have gotten these results?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "#Please write your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Here, please write a few sentences to explain your results:\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
