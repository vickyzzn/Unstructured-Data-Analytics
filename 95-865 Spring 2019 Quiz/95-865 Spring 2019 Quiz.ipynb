{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 95-865 Spring 2019 Quiz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your name: Zhining Zhou\n",
    "\n",
    "Your Andrew ID: zhiningz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This quiz has two problems, a conceptual problem (Problem 1), and a coding problem (Problem 2)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1: Conceptual questions [45 points]\n",
    "\n",
    "Parts (a), (b), and (c) can be done in any order. For this problem, your answers should *not* involve any coding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (a): True or false, and explain [9 points]\n",
    "\n",
    "For each of the statements below, please specify whether the statement is true or false, *and then briefly explain why*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(i) When running k-means with 100 random initializations and a fixed choice of the number of clusters k, the best clustering is found using CH index."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer here**: False. CH index is used to choose the number of clusters (varying k). Having fixed number of clusters k, you cannot know the best clustering using CH index. If you have different choices among k, CH index can help you choose the number of clusters you should have. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(ii) If we run divisive clustering using k-means (with k=2) to do each split, and we stop after we have 3 clusters, the result is the same as if we fit k-means with 3 clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer here**: False. For divisive clustering, each iteration you split only in one of the clusters that have the largest variance within the cluster. In k-means, all 3 clusters can change at one iteration until convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(iii) Suppose two entities have a high PMI (much larger than 0) and they only co-occur once. This could be because one of the entities is extremely uncommon (i.e., has a low probability of appearing)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer here**: True. PMI = log((P(A,B)/P(A)P(B)) Having a high PMI when P(A,B) is small could be that P(A)P(B) is extremely small. It could be that one entity has low probability of occuring. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part (b): Bigrams [8 points]\n",
    "\n",
    "Consider the following string: \"21100322232\"\n",
    "\n",
    "Treating each character as a separate token, determine the histogram of raw counts of the bigrams in the string above (by hand and not using any code). Remove any bigram term that has the stop word character \"1\" anywhere in it. Your answer should be of the format:\n",
    "\n",
    "```\n",
    "bigram term 1: raw count for bigram term 1\n",
    "bigram term 2: raw count for bigram term 2\n",
    "...\n",
    "```\n",
    "\n",
    "Please present your bigram terms in *increasing* order (rather than decreasing order) of raw count (break ties arbitrarily)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer here (no code)**:\n",
    "\n",
    "bigram term 00: 1<br>\n",
    "bigram term 03: 1<br>\n",
    "bigram term 23: 1<br>\n",
    "bigram term 22: 2<br>\n",
    "bigram term 32: 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (c): Isomap [28 points]\n",
    "\n",
    "Consider 5 points A, B, C, D, and E with the following table of Euclidean distances between them (these do *not* account for any sort of nearest neighbor graph structure):\n",
    "\n",
    "| &nbsp; | A          | B          | C          | D          | E          |\n",
    "| ------ | ---------- | ---------- | ---------- | ---------- | ---------- |\n",
    "| A      | 0          | 1          | $\\sqrt{2}$ | $\\sqrt{5}$ | 2          |\n",
    "| B      | 1          | 0          | 1          | 2          | $\\sqrt{5}$ |\n",
    "| C      | $\\sqrt{2}$ | 1          | 0          | 1          | $\\sqrt{2}$ |\n",
    "| D      | $\\sqrt{5}$ | 2          | 1          | 0          | 1          |\n",
    "| E      | 2          | $\\sqrt{5}$ | $\\sqrt{2}$ | 1          | 0          |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(i) What is the single nearest neighbor of point A?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer here**: B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(ii) What are the two nearest neighbors of point B?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer here**: A, C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(iii) What are the two nearest neighbors of point C?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer here**: B, D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(iv) What are the two nearest neighbors of point D?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer here**: C, E"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(v) What is the single nearest neighbor of point E?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer here**: D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now suppose that we construct a nearest neighbor graph as follows:\n",
    "\n",
    "- We connect point A to its single nearest neighbor.\n",
    "- We connect points B, C, and D each to its two nearest neighbors.\n",
    "- We connect point E to its single nearest neighbor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(vi) What are the edges (\"roads\") that get constructed? Write your answer by listing out the edges that get constructed (you can specify an edge using the phrase \"(A,E)\" to mean an edge between point A and point E; note that edge (A,E) and (E,A) are the same so you do not need to write both of them)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer here**: (A, B), (B, C), (C, D), (D, E)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(vii) Now, using the edges you have listed down in the previous step, compute the shortest distances between every pair of points where one can only travel on the edges (\"roads\")."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Specify your answer by filling out the table below:**\n",
    "\n",
    "| &nbsp; | A          | B          | C          | D          | E          |\n",
    "| ------ | ---------- | ---------- | ---------- | ---------- | ---------- |\n",
    "| A      | 0          | 1          | 2          | 3          | 4          |\n",
    "| B      | 1          | 0          | 1          | 2          | 3          |\n",
    "| C      | 2          | 1          | 0          | 1          | 2          |\n",
    "| D      | 3          | 2          | 1          | 0          | 1          |\n",
    "| E      | 4          | 3          | 2          | 1          | 0          |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(viii) What is a valid one-dimensional representation of the points A, B, C, D, and E that would have Euclidean distances (in 1D) exactly match the table you computed in subpart (v), and where the low-dimensional coordinate for A is A' = 0? As a reminder, there is no coding in this problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Specify your answer by writing in values for points B', C', D', E' (low-dimensional versions of B, C, D, E):**\n",
    "\n",
    "B' = 1\n",
    "\n",
    "C' = 2\n",
    "\n",
    "D' = 3\n",
    "\n",
    "E' = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(ix) Is there more than 1 valid answer to subpart (vi) in which A' = 0? Why or why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer here**: Yes. B', C', D', E' could be to the right side of A' (as in previous section) while they could also be to the left of A' such as B'=-1, C'=-2, D'=-3, E'=-4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2: Analyzing Judge Opinions [55 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this problem, we examine a dataset of majority opinions written by U.S. courts from 1970-2018. While the judges' votes determine the outcomes of each case, the written majority opinions (in this dataset) sets the scope and justification of the precedent that the immediate ruling establishes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code cell below reads in the data and stores it in two separate lists:\n",
    "\n",
    "- The opinion text (stored in the Python variable `opinions`)\n",
    "- The judgement year (stored in `years`)\n",
    "\n",
    "The opinion texts have already been preprocessed and lemmatized.\n",
    "\n",
    "**Important: Be sure to run the cell below before you run anything else.** *After running the cell below, you can do parts (a) and (b) in either order.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4388\n",
      "4388\n"
     ]
    }
   ],
   "source": [
    "# DO NOT MODIFY THIS CELL\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "plt.style.use('seaborn')\n",
    "\n",
    "years = []\n",
    "opinions = []\n",
    "with open('opinions.txt', 'r', encoding='UTF-8') as f:\n",
    "    for line in f:\n",
    "        opinions.append(line.strip())\n",
    "with open('years.txt', 'r', encoding='UTF-8') as f:\n",
    "    for line in f:\n",
    "        years.append(int(line.strip()))\n",
    "years = np.array(years)\n",
    "\n",
    "print(len(opinions)) # should be 4388\n",
    "print(len(years)) # should be 4388"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part (a): Extracting Topics with LDA [30 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subpart 2(a)-i: Construct the TF-IDF matrix [10 points]\n",
    "\n",
    "1. Remove the most common 100 words and store the final vocabulary in a list called `vocabulary` (you should find that there are 106723 words after removing the most common 100). After removing the most common 100 words, make sure to sort the words alphabetically.\n",
    "2. Construct a (4388, 106723) matrix of TF-IDF scores for the opinions using this `vocabulary`. Store this matrix as `tfidf_mat`. (In particular, you should use `TfidfVectorizer` with a single parameter `vocabulary=vocabulary`.)\n",
    "\n",
    "Note: for us, doing the above two steps takes less than 30 seconds to run. If you simply cannot get these to run correctly, you can load in the final values using the code at the beginning of Problem 2(b). We will grade subpart 2(a)-i based on whether you can correctly produce `vocabulary` and `tfidf_mat` *without* using our pre-computed values in Problem 2(b)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4388, 106723)\n"
     ]
    }
   ],
   "source": [
    "words = [word.lower() for opinion in opinions for word in opinion.split()] # these are all the words\n",
    "\n",
    "# WARNING: do *not* use spaCy and do *not* remove any words except for the most common 100\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# YOUR CODE HERE\n",
    "#\n",
    "from collections import Counter\n",
    "\n",
    "histogram = Counter()\n",
    "for word in words:\n",
    "    histogram[word] += 1\n",
    "\n",
    "vocabulary = []\n",
    "lemmas_unsorted = []\n",
    "counts_unsorted = []\n",
    "for lemma, count in histogram.items():\n",
    "    lemmas_unsorted.append(lemma)\n",
    "    counts_unsorted.append(count)\n",
    "    \n",
    "counts_unsorted = np.array(counts_unsorted) \n",
    "sort_indices = np.argsort(counts_unsorted)[::-1]\n",
    "\n",
    "indices = sort_indices[100:]\n",
    "for i in indices:\n",
    "    vocabulary.append(lemmas_unsorted[i])\n",
    "vocabulary.sort()\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(vocabulary=vocabulary)\n",
    "tfidf_mat = vectorizer.fit_transform(opinions)  \n",
    "\n",
    "print(tfidf_mat.shape)\n",
    "\n",
    "\n",
    "\n",
    "#\n",
    "# END OF YOUR CODE\n",
    "# ------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subpart 2(a)-ii: Extract 10 topics using LDA [5 points]\n",
    "\n",
    "We have already fitted an LDA model to the data for you (because this is slow). Using the fitted LDA components learned, print out the top 10 words for each topic. To keep the printing concise, please have your output displayed in the following format:\n",
    "\n",
    "```\n",
    "Topic 0 : list of the top 10 words separated by commas, where the first word is the most probable for the topic\n",
    "Topic 1 : ...\n",
    "Topic 2 : ...\n",
    "...\n",
    "Topic 9 : ...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "106723"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lda_components[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0:  ['fmcsa', 'newsracks', 'plasma', 'hyphenate', 'racetrack', 'persecutor', 'cantaloupe', 'letterbox', 'sdn', 'eulogio']\n",
      "\n",
      "Topic #1:  ['piccadilly', 'pedal', 'ssdi', 'nevils', 'equilon', 'ttab', 'mirzayance', 'sto', 'stos', 'hastings']\n",
      "\n",
      "Topic #2:  ['cfc', 'copa', 'detainers', 'tcpa', 'overmyer', 'cognovit', 'cheever', 'winery', 'santana', 'pelly']\n",
      "\n",
      "Topic #3:  ['sorna', 'subclause', 'hoeper', 'nonregistrants', 'coupler', 'din', 'nelsons', 'pregranted', 'fryer', 'burro']\n",
      "\n",
      "Topic #4:  ['olympic', 'chick', 'fog', 'broiler', 'rumery', 'prescriber', 'mms', 'gme', 'detailers', 'repo']\n",
      "\n",
      "Topic #5:  ['first', 'counsel', 'fourth', 'employer', 'plan', 'agency', 'immunity', 'officer', 'plaintiff', 'suit']\n",
      "\n",
      "Topic #6:  ['escheat', 'haskell', 'ilecs', 'guerrilla', 'rrha', 'jackpot', 'scr', 'misjoinder', 'vartelas', 'thompkins']\n",
      "\n",
      "Topic #7:  ['delawares', '16c', 'sfts', 'exeat', 'postcrime', 'aetc', '47b', 'sag', 'trest', 'lust']\n",
      "\n",
      "Topic #8:  ['iras', 'boren', 'stumpf', 'asme', 'honorarium', 'tovar', 'ocp', 'creche', 'maples', 'deadhead']\n",
      "\n",
      "Topic #9:  ['methadone', 'lodger', 'shale', 'worthen', 'toca', 'allocatur', 'sulindac', 'neomycin', 'nonunitary', '1514a']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lda_components = np.loadtxt('lda_components.txt')  # pre-fitted LDA components\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# YOUR CODE HERE\n",
    "#\n",
    "n_top_words = 10\n",
    "feature_names = vocabulary\n",
    "most_probable_words = []\n",
    "for topic_idx, topic in enumerate(lda_components):\n",
    "    message = \"Topic #%d: \" % topic_idx\n",
    "    indices = topic.argsort()[:-n_top_words-1:-1]\n",
    "    for i in indices:\n",
    "        most_probable_words.append(feature_names[i])\n",
    "    print(message, most_probable_words)\n",
    "    most_probable_words = []\n",
    "    print() \n",
    "\n",
    "#\n",
    "# END OF YOUR CODE\n",
    "# ------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subpart 2(a)-iii: Rank the top 10 words in each topic using \"relevance\" [15 points]\n",
    "\n",
    "\"*Relevance*\" is as an alternate metric to rank words so as to penalize frequent words. This sometimes results in better descriptions of topics than ranking words by the word-topic probabilities as in 2(a)-ii. The relevance of a word $w$ for each topic $k$ is defined as:\n",
    "\n",
    "$$\n",
    "r(k, w) = \\lambda \\textrm{log}(P(w|k)) + (1 - \\lambda)\\textrm{log}(\\frac{P(w|k)}{P(w)})\n",
    "$$\n",
    "\n",
    "Here, $P(w|k)$ is the probability of word $w$ given the topic $k$, and $P(w)$ is the overall probability of the word in the corpus.\n",
    "\n",
    "**Print the top 10 words in each topic ranked by relevance for $\\lambda =$ 0.0, 0.4, 0.8 and 1.0.**\n",
    "\n",
    "Note:\n",
    "   - You will be printing 4 word lists, each one similar to the one you printed in 2(a)-ii.\n",
    "   - When $\\lambda = 1$, the words are ranked by $P(w|k)$ as usual and your results should be identical to 2(a)-ii.\n",
    "   - You must *reuse* the fitted LDA components (do not fit a new LDA model). We are only changing the way the top words of a topic are ranked.\n",
    "\n",
    "*Hint to compute $P(w)$:*\n",
    "\n",
    "To compute $P(w)$, make use of the raw counts you computed in subpart 2(a)-i. Divide by the total sum of raw counts across words that didn't get removed.\n",
    "\n",
    "*Hints to compute $P(w|k)$:*\n",
    "\n",
    "You already have a matrix containing the *unnormalized* $P(w|k)$; this needs to be normalized by making sure each row sums to one.\n",
    "\n",
    "**Important:** It's okay if you find that the topics are not very interpretable. We will be grading based on whether you can do the calculations mentioned above correctly rather than whether the topics are interpretable. In part (b), we will be looking at an approach that results in more interpretable topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------------\n",
    "# YOUR CODE HERE (to compute P(w))\n",
    "#\n",
    "\n",
    "P_w = np.zeros(len(vocabulary)) # fill/replace this 106723-length array with P(w)\n",
    "\n",
    "sum_words = 0\n",
    "for i in vocabulary:\n",
    "    sum_words += histogram[i]\n",
    "\n",
    "    \n",
    "for i in range(len(vocabulary)):\n",
    "    P_w[i] = histogram[vocabulary[i]] / sum_words\n",
    "\n",
    "#\n",
    "# END OF YOUR CODE\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# YOUR CODE HERE (to compute P(w|k))\n",
    "#\n",
    "\n",
    "P_wk = np.zeros((10, len(vocabulary))) # replace this 10 x 106723 matrix with P(w|k)\n",
    "sum_wk = 0\n",
    "for i in range(10):\n",
    "    sum_wk = np.sum(lda_components[i])\n",
    "    for j in range(len(vocabulary)):\n",
    "        P_wk[i][j] = lda_components[i][j] / sum_wk\n",
    "#\n",
    "# END OF YOUR CODE\n",
    "# ------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[lambda = 0]\n",
      "Topic #0:  ['bpas', 'projections', 'geneticist', 'berd', 'cytosine', 'mutational', 'cleotides', 'guanine', 'adenine', 'polypeptide']\n",
      "Topic #1:  ['taliento', 'crossappeal', 'forney', 'hastings', 'juryless', 'dicative', 'federated', 'leakproof', 'fastners', 'opposer']\n",
      "Topic #2:  ['hargrave', 'ballew', 'gilletti', 'bellows', 'weimer', 'bowlus', 'polyethylene', 'causé', 'arrivé', 'subie']\n",
      "Topic #3:  ['piercing', 'oblong', 'andpin', 'railworkers', 'misalignment', 'hiles', 'switchyard', 'fingertip', 'coupling', 'tubelike']\n",
      "Topic #4:  ['rheumatoid', 'retinopathy', 'angiogenic', 'antagonists', 'tripeptide', 'lionel', 'hospitalspecific', 'timeequivalent', 'baseyear', 'andleaseback']\n",
      "Topic #5:  ['10', '18', 'rep', '15', '23', 'ny', 'them', 'itself', 'uch', 'th']\n",
      "Topic #6:  ['unclassiflable', 'postconstruction', 'teck', 'preorder', 'logistically', 'contradicting', 'towed', 'stroller', 'skater', 'discloseable']\n",
      "Topic #7:  ['spomer', 'berbling', 'napalm', 'descendancy', 'famca', 'timothy', 'ploited', 'abductions', 'oct', 'astronomy']\n",
      "Topic #8:  ['maples', 'mckown', 'carveouts', 'postplea', 'nonmurder', 'needing', 'offduty', 'cair', '1857a', 'tributing']\n",
      "Topic #9:  ['fulcomer', 'semipublic', 'busdrivers', 'detoxify', 'pleasurable', 'towermen', 'cinnabar', 'bartolis', 'leaseable', 'nonadopting']\n",
      "\n",
      "[lambda = 0.4]\n",
      "Topic #0:  ['machin', 'riverboat', 'riverboats', 'unlv', 'racetrack', 'cantaloupe', 'eulogio', 'projections', 'bpas', 'driehaus']\n",
      "Topic #1:  ['taliento', 'hastings', 'forney', 'crossappeal', 'ultrahazardous', 'equilon', 'sonic', 'nevils', 'simmonds', 'nonventure']\n",
      "Topic #2:  ['gilletti', '49b', 'nonunanimous', 'sabic', 'hargrave', 'ballew', 'exxonmobil', 'nonpetty', 'polyethylene', 'avendano']\n",
      "Topic #3:  ['piercing', 'tru', 'gourlay', 'wombwell', 'pregranted', 'osterholme', 'yarmulke', 'nonregistrants', 'wortman', 'nelsons']\n",
      "Topic #4:  ['lionel', '437h', 'cftb', 'firstier', '1881a', 'mariana', 'rumery', 'ophthalmological', 'wapa', 'chick']\n",
      "Topic #5:  ['10', 'rep', '15', '18', '23', 'ny', '11', '14', 'them', 'dr']\n",
      "Topic #6:  ['jiffy', 'oba', 'westchester', 'molina', 'melinger', 'laroe', 'concessioners', 'imds', 'guerrilla', 'concessioner']\n",
      "Topic #7:  ['napalm', 'berbling', 'spomer', 'trest', 'sfts', 'chairlady', '16c', 'postcrime', 'tarazon', 'dumper']\n",
      "Topic #8:  ['maples', 'ritz', 'ocp', 'regulatee', 'regulatees', 'holmquist', 'mti', 'tdca', 'desked', 'remailing']\n",
      "Topic #9:  ['allocatur', 'fulcomer', 'propanone', 'personhuballah', 'wittman', 'ohler', 'neomycin', '1514a', 'cantu', 'cvp']\n",
      "\n",
      "[lambda = 0.8]\n",
      "Topic #0:  ['newsracks', 'plasma', 'racetrack', 'fmcsa', 'cantaloupe', 'hyphenate', 'unlv', 'eulogio', 'sdn', 'persecutor']\n",
      "Topic #1:  ['ssdi', 'hastings', 'piccadilly', 'nevils', 'equilon', 'taliento', 'pedal', 'sto', 'ultrahazardous', 'stos']\n",
      "Topic #2:  ['cfc', 'copa', 'detainers', 'tcpa', 'overmyer', 'cognovit', 'cheever', 'nonunanimous', '49b', 'nonpetty']\n",
      "Topic #3:  ['sorna', 'nonregistrants', 'subclause', 'coupler', 'hoeper', 'nelsons', 'pregranted', 'din', 'osterholme', 'yarmulke']\n",
      "Topic #4:  ['chick', 'rumery', 'olympic', 'fog', 'mms', 'broiler', 'gme', '437h', 'maurice', 'repo']\n",
      "Topic #5:  ['rep', '10', 'first', 'fourth', 'immunity', 'employer', 'counsel', 'plan', 'fee', 'agency']\n",
      "Topic #6:  ['escheat', 'guerrilla', 'haskell', 'rrha', 'ilecs', 'jiffy', 'jackpot', 'vartelas', 'imds', 'acquittee']\n",
      "Topic #7:  ['sfts', 'delawares', '16c', 'postcrime', 'napalm', 'trest', 'aetc', 'exeat', 'berbling', '47b']\n",
      "Topic #8:  ['maples', 'iras', 'boren', 'ocp', 'stumpf', 'asme', 'tovar', 'creche', 'honorarium', 'deadhead']\n",
      "Topic #9:  ['methadone', 'allocatur', 'lodger', 'toca', 'neomycin', 'shale', '1514a', 'cantu', 'worthen', 'betterman']\n",
      "\n",
      "[lambda = 1.0]\n",
      "Topic #0:  ['fmcsa', 'newsracks', 'plasma', 'hyphenate', 'racetrack', 'persecutor', 'cantaloupe', 'letterbox', 'sdn', 'eulogio']\n",
      "Topic #1:  ['piccadilly', 'pedal', 'ssdi', 'nevils', 'equilon', 'ttab', 'mirzayance', 'sto', 'stos', 'hastings']\n",
      "Topic #2:  ['cfc', 'copa', 'detainers', 'tcpa', 'overmyer', 'cognovit', 'cheever', 'winery', 'santana', 'pelly']\n",
      "Topic #3:  ['sorna', 'subclause', 'hoeper', 'nonregistrants', 'coupler', 'din', 'nelsons', 'pregranted', 'fryer', 'burro']\n",
      "Topic #4:  ['olympic', 'chick', 'fog', 'broiler', 'rumery', 'prescriber', 'mms', 'gme', 'detailers', 'repo']\n",
      "Topic #5:  ['first', 'counsel', 'fourth', 'employer', 'plan', 'agency', 'immunity', 'officer', 'plaintiff', 'suit']\n",
      "Topic #6:  ['escheat', 'haskell', 'ilecs', 'guerrilla', 'rrha', 'jackpot', 'scr', 'misjoinder', 'vartelas', 'thompkins']\n",
      "Topic #7:  ['delawares', '16c', 'sfts', 'exeat', 'postcrime', 'aetc', '47b', 'sag', 'trest', 'lust']\n",
      "Topic #8:  ['iras', 'boren', 'stumpf', 'asme', 'honorarium', 'tovar', 'ocp', 'creche', 'maples', 'deadhead']\n",
      "Topic #9:  ['methadone', 'lodger', 'shale', 'worthen', 'toca', 'allocatur', 'sulindac', 'neomycin', 'nonunitary', '1514a']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for lam in [0, 0.4, 0.8, 1.0]:\n",
    "    print('[lambda = ', lam, ']', sep='')\n",
    "    \n",
    "    # Print the top 10 words by reusing your code in 2(a)-ii\n",
    "    # The only change required is to sort by relevance, instead of\n",
    "    # unnormalized word probabilities in each topic\n",
    "    \n",
    "    # --------------------------------------------------------------------------\n",
    "    # YOUR CODE HERE\n",
    "    #\n",
    "    \n",
    "    # 1. compute relevance scores\n",
    "    scores = lam * np.log(P_wk) + (1 - lam) * np.log(P_wk / P_w)\n",
    "    \n",
    "    # 2. print the top 10 words per topic sorted by relevance\n",
    "    n_top_words = 10\n",
    "    feature_names = vocabulary\n",
    "    most_probable_words = []\n",
    "    for topic_idx, topic in enumerate(lda_components):\n",
    "        message = \"Topic #%d: \" % topic_idx\n",
    "        indices = scores[topic_idx].argsort()[:-n_top_words-1:-1]\n",
    "        for i in indices:\n",
    "            most_probable_words.append(feature_names[i])\n",
    "        print(message, most_probable_words)\n",
    "        most_probable_words = []\n",
    "\n",
    "    #\n",
    "    # END OF YOUR CODE\n",
    "    # --------------------------------------------------------------------------\n",
    "    \n",
    "    print()  # empty new line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part (b): Extracting Topics with LSA and Clustering [25 points]\n",
    "\n",
    "Latent Semantic Analysis (LSA) is a dimensionality reduction method similar to PCA. For the purposes of this problem, you do not need to know what LSA does. We have already reduced the dimensionality of the dataset using LSA. Now, as an alternative to LDA, we extract topics by clustering this low-dimensional LSA representation of the data. *In particular, we will interpret the resulting clusters to be the topics.*\n",
    "\n",
    "The code below loads the LSA low-dimensional representation of the original data into the variable `lsa_mat`. The code then also loads the TF-IDF matrix of the documents as `tfidf_mat`, along with the vocabulary as `vocabulary2`. If you correctly constructed these in 2(a)-i, they should be identical. **Be sure to run the cell below before proceeding to subsequent subparts.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4388, 300)\n",
      "(4388, 106723)\n",
      "106723\n"
     ]
    }
   ],
   "source": [
    "# DO NOT MODIFY THIS CELL\n",
    "lsa_mat = np.loadtxt(\"lsa_mat.csv\", delimiter=\",\")   # LSA low-dimensional representation of the data\n",
    "print(lsa_mat.shape) # should be (4388, 300)\n",
    "\n",
    "# the code below loads precomputed versions of the correct output to subpart 2(a)-i\n",
    "import scipy.sparse\n",
    "tfidf_mat = scipy.sparse.load_npz('tfidf.npz')\n",
    "vocabulary2 = [word.strip() for word in open('vocabulary.txt', 'r', encoding='UTF-8').readlines()]\n",
    "print(tfidf_mat.shape)\n",
    "print(len(vocabulary2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subpart 2(b)-i: Cluster the LSA matrix using k-means [5 points]\n",
    "\n",
    "Set the number of clusters to 10 and `random_state=11`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------------\n",
    "# YOUR CODE HERE\n",
    "#\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "kmeans = KMeans(n_clusters=10,random_state=11)\n",
    "kmeans.fit(lsa_mat)\n",
    "\n",
    "cluster_assignments = kmeans.predict(lsa_mat)\n",
    "#\n",
    "# END OF YOUR CODE\n",
    "# -----------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subpart 2(b)-ii: Construct \"topics\" using the cluster labels [10 points]\n",
    "\n",
    "Remember how in lecture, for each cluster, we computed the average feature vector for that cluster? We now build on this idea.\n",
    "\n",
    "For each cluster $k$:\n",
    "\n",
    "   - Compute the average TF-IDF row (this is like the average feature vector) for all documents inside cluster $k$: call this array `mean_tf_idf_vector`. (Recall that the feature vectors in our current setup is stored in `tfidf_mat`; we have converted this into a 2D numpy array `tfidf_mat_as_numpy_array` for you.)\n",
    "   - Print out the top 10 words (using `vocabulary2`) for the \"topic\" of cluster $k$ by sorting `mean_tf_idf_vector` in decreasing order.\n",
    "\n",
    "Your final printed output should be of the following format:\n",
    "\n",
    "```\n",
    "Topic 0 : list of the top 10 words separated by commas, where the first word is the most probable for the topic\n",
    "Topic 1 : ...\n",
    "Topic 2 : ...\n",
    "...\n",
    "Topic 9 : ...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'first'"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.mean(tfidf_mat_as_numpy_array[cluster_assignments==1], axis=0)\n",
    "indices = np.argsort(x)[::-1][:10]\n",
    "vocabulary2[45711]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0:  ['union', 'arbitration', 'bargain', 'agreement', 'collective', 'employer', 'labor', 'contract', 'arbitrator', 'dispute']\n",
      "\n",
      "Topic #1:  ['first', 'speech', 'candidate', 'political', 'ordinance', 'election', 'advertise', 'protect', 'obscene', 'expression']\n",
      "\n",
      "Topic #2:  ['plan', 'agency', 'immunity', 'suit', 'plaintiff', 'damage', 'employer', 'discrimination', 'secretary', 'employment']\n",
      "\n",
      "Topic #3:  ['counsel', 'conviction', 'habeas', 'offense', 'death', 'miranda', 'crime', 'sixth', 'murder', 'juror']\n",
      "\n",
      "Topic #4:  ['debtor', 'bankruptcy', 'creditor', 'debt', 'lien', 'estate', 'plan', 'priority', 'payment', 'discharge']\n",
      "\n",
      "Topic #5:  ['fee', 'attorney', 'award', 'prevail', 'cost', 'plaintiff', 'litigation', 'civil', 'eaja', 'settlement']\n",
      "\n",
      "Topic #6:  ['school', 'religious', 'student', 'child', 'education', 'religion', 'teacher', 'program', 'first', 'educational']\n",
      "\n",
      "Topic #7:  ['price', 'income', 'patent', 'sale', 'bank', 'market', 'rate', 'commerce', 'business', 'interstate']\n",
      "\n",
      "Topic #8:  ['fourth', 'search', 'officer', 'arrest', 'police', 'warrant', 'seizure', 'probable', 'stop', 'privacy']\n",
      "\n",
      "Topic #9:  ['indian', 'land', 'water', 'reservation', 'tribal', 'tribe', 'river', 'boundary', 'secretary', 'title']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tfidf_mat_as_numpy_array = np.asarray(tfidf_mat.todense())\n",
    "\n",
    "top_words_lsa = {}  # this will be helpful for subpart 2(b)-iii\n",
    "top_words = []\n",
    "for i in range(10):\n",
    "    # -------------------------------------------------------------------------\n",
    "    # YOUR CODE HERE\n",
    "    #\n",
    "    mean_tf_idf_vector = np.mean(tfidf_mat_as_numpy_array[cluster_assignments==i], axis=0)  # fill this out correctly making use of `tfidf_mat_as_numpy_array`\n",
    "    indices = np.argsort(mean_tf_idf_vector)[::-1][:10]\n",
    "    for j in indices:\n",
    "        top_words.append(vocabulary2[j])\n",
    "    message = \"Topic #%d: \" % i    \n",
    "    top_words_lsa[i] = top_words  # DO NOT MODIFY THIS LINE\n",
    "    print(message, top_words)\n",
    "    print()\n",
    "    top_words = []\n",
    "    # add your printing code here\n",
    "    \n",
    "    #\n",
    "    # END OF YOUR CODE\n",
    "    # -------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subpart 2(b)-iii: Plot the prevalence of the LSA topics over time [10 points]\n",
    "\n",
    "Each LSA cluster corresponds to a topic.\n",
    "\n",
    "   1. Use the `years` array to plot how frequently each topic appears over time. In particular, for this plot, please let the frequency of a topic in a year be the *fraction* of documents having that topic in that year. There should be one line for each topic. *Also, make sure that in your plot, you have the years sorted chronologically. Each year should of course only appear once in the plot.*\n",
    "   2. Label the line for each topic with its 3 top words (using the topic-words dictionary `top_words_lsa` in subpart 2(b)-ii. Make sure you plot the legend to display these labels. (We suggest using `plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))`.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------------\n",
    "# YOUR CODE HERE\n",
    "#\n",
    "\n",
    "#\n",
    "# END OF YOUR CODE\n",
    "# -----------------------------------------------------------------------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
