{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UDA Midterm Spring 2018 B4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations! You have been tasked with finding out how news headlines change across months in year 2015!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Read the data [10 points]\n",
    "\n",
    "Begin by reading in the headlines from the file `headlines.csv` (which should be in the same folder as this Jupyter notebook).\n",
    "\n",
    "The first line of the data contains the column headers `publish_date` and `headline_text`. Each line that follows contains the publish date in `YYYYMMDD` format and the headline, separated by a comma. The first few lines of the data look like this:\n",
    "```\n",
    "publish_date,headline_text\n",
    "20050101,10 killed in kashmir new year party called off\n",
    "20050101,aftershocks continue to rattle aceh\n",
    "20050101,aid for some aceh survivors weeks away\n",
    "20050101,aid pours in to indonesia\n",
    "20050101,american gordon makes dakar rally history\n",
    "20050101,ancic llodra out of hardcourt champs\n",
    "20050101,archives reveal act education boost\n",
    "20050101,archives reveal qld concerns over colour tv\n",
    "20050101,army choppers to aid relief efforts\n",
    "```\n",
    "\n",
    "Tasks:\n",
    "\n",
    "   1. Read the headlines into a list of strings named `headlines`.\n",
    "   2. Read the publish dates into a list of strings named `publish_dates` (for now we just leave these as strings).\n",
    "\n",
    "Hints:\n",
    "\n",
    "   * Make sure you ignore the header line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "headlines = []\n",
    "publish_dates = []\n",
    "################################################################################\n",
    "# YOUR CODE GOES HERE\n",
    "################################################################################\n",
    "idx = 0;\n",
    "with open('headlines.csv', 'r') as f:\n",
    "    for line in f:\n",
    "        lines = line.strip().split(',')\n",
    "        headlines.append(lines[1])\n",
    "        publish_dates.append(lines[0])\n",
    "\n",
    "    \n",
    "headlines = headlines[1:]\n",
    "publish_dates = publish_dates[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73124\n",
      "73124\n"
     ]
    }
   ],
   "source": [
    "print(len(headlines)) # should be 73124\n",
    "print(len(publish_dates)) # should be 73124"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preprocess the data [30 points]\n",
    "\n",
    "Using the `headlines` and `publish_dates` lists created above, perform the following in order:\n",
    "\n",
    "   1. [15 points] Delete duplicate headlines and their corresponding publish dates (make sure to maintain the association between each headline and publish date). In particular, if a headline appears more than once, only keep the first instance of it being seen in the list `headlines`.\n",
    "   2. [15 points] Delete stopwords (as specified by spacy's stopwords) from each headline. (We've provided code that imports this stopwords list.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "deduplicated_headlines = []\n",
    "deduplicated_publish_dates = []\n",
    "\n",
    "################################################################################\n",
    "# YOUR CODE GOES HERE\n",
    "################################################################################\n",
    "for i in range(len(headlines)):\n",
    "    if not (headlines[i] in deduplicated_headlines):\n",
    "        deduplicated_headlines.append(headlines[i])\n",
    "        deduplicated_publish_dates.append(publish_dates[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72915\n",
      "72915\n"
     ]
    }
   ],
   "source": [
    "print(len(deduplicated_headlines)) # should be 72915\n",
    "print(len(deduplicated_publish_dates)) # should be 72915\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "\n",
    "processed_headlines = []\n",
    "\n",
    "################################################################################\n",
    "# YOUR CODE GOES HERE\n",
    "################################################################################\n",
    "for i in range(len(deduplicated_headlines)):\n",
    "    headline = deduplicated_headlines[i].split(' ')\n",
    "    li = []\n",
    "    for j in range(len(headline)):\n",
    "        if not (headline[j] in STOP_WORDS):\n",
    "            li.append(headline[j])\n",
    "    processed_headlines.append(\" \".join(str(x) for x in li))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a last step of preprocessing, we represent each headline as a feature vector, very similar to building a histogram. We have provided this code for you. The code uses scikit-learn's `TfidfVectorizer` class, which, similar to `CountVectorizer` in the LDA demo, represents each document as a feature vector (for this exam, each document corresponds to a headline). The resulting variable `X` is a 2-D numpy array, and has rows corresponding to headlines, and columns corresponding to different words. However, the main difference from `CountVectorizer` is that each feature vector value is not just the raw count of how many times a word appeared. Instead, it's the raw count multiplied by a weighting factor (so that \"unimportant\" words get downweighted more). For the purposes of this exam, you do *not* have to worry about how this weighting works. Just treat `X` as a 2D numpy array where rows are feature vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(72915, 5460)\n"
     ]
    }
   ],
   "source": [
    "# *** DO NOT MODIFY CODE IN THIS CELL ***\n",
    "# If your code above works correctly, then running this cell should output (72915, 5460).\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(min_df=10, max_df=0.75)\n",
    "X = vectorizer.fit_transform(processed_headlines)  # This is stored as a sparse array, so as to save memory\n",
    "                                                   # (very similar to CountVectorizer in the LDA demo!)\n",
    "\n",
    "print(X.shape) # should be (72915, 5460)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Cluster the headlines with k-means [20 points]\n",
    "\n",
    "a. [10 points] In this problem, we use a variant of k-means called mini-batch k-means, which is similar to regular k-Means clustering, but performs faster. Just like with regular k-means, after creating an instance of the `MiniBatchKMeans` class (suppose we call this instance `kmeans`), you use `kmeans.fit` to run mini-batch k-means on the data you would like to cluster. You can then get cluster assignments of the data points using `kmeans.labels_`, and you can get the cluster centers by using `kmeans.cluster_centers_`.\n",
    "\n",
    "The goal of this problem is to figure out what the value of `k` is according to the so-called [\"silhouette score\"](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.silhouette_score.html#sklearn.metrics.silhouette_score) (note: you don't need to know the details of what this score is computing!--for the purposes of this exam, you just need to know that a higher silhouette score is better).\n",
    "\n",
    "In particular, for each k=10,20,30,...,100: cluster all the headlines (in full-dimensionality, so don't reduce dimensions!) into k clusters, and compute the silhouette score for this choice of k.\n",
    "\n",
    "This should take less than 2 minutes to run (for all `k`) and consumes < 2.5GB of RAM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k = 10 has silhouette score 0.0020085944128735403\n",
      "k = 10 has silhouette_score 0.0020085944128735403\n",
      "k = 20 has silhouette score -0.0035662599628416333\n",
      "k = 20 has silhouette_score -0.0035662599628416333\n",
      "k = 30 has silhouette score -0.010989179128828705\n",
      "k = 30 has silhouette_score -0.010989179128828705\n",
      "k = 40 has silhouette score -0.015047750994709761\n",
      "k = 40 has silhouette_score -0.015047750994709761\n",
      "k = 50 has silhouette score -0.012605691095263041\n",
      "k = 50 has silhouette_score -0.012605691095263041\n",
      "k = 60 has silhouette score -0.015382504192146672\n",
      "k = 60 has silhouette_score -0.015382504192146672\n",
      "k = 70 has silhouette score -0.006099793648054312\n",
      "k = 70 has silhouette_score -0.006099793648054312\n",
      "k = 80 has silhouette score -0.002408741517043712\n",
      "k = 80 has silhouette_score -0.002408741517043712\n",
      "k = 90 has silhouette score -0.006051450907709904\n",
      "k = 90 has silhouette_score -0.006051450907709904\n",
      "k = 100 has silhouette score -0.006126304271135071\n",
      "k = 100 has silhouette_score -0.006126304271135071\n",
      "Best k = 10\n",
      "CPU times: user 54.5 s, sys: 6.29 s, total: 1min\n",
      "Wall time: 38.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "best_k = None\n",
    "best_cluster_centers = None\n",
    "best_labels = None\n",
    "best_score = -1\n",
    "for k in range(10, 110, 10):\n",
    "    kmeans = MiniBatchKMeans(n_clusters=k, random_state=95865, verbose=0)\n",
    "    \n",
    "    ############################################################################\n",
    "    # YOUR CODE HERE (cluster data using the kmeans object created above)\n",
    "    ############################################################################\n",
    "    kmeans.fit(X)\n",
    "    \n",
    "    labels = kmeans.labels_\n",
    "    score = silhouette_score(X, labels, sample_size=10000)\n",
    "    print('k =', k, 'has silhouette score', score)\n",
    "    \n",
    "    if score > best_score:\n",
    "        best_score = score\n",
    "        best_k = k\n",
    "        best_cluster_centers = kmeans.cluster_centers_\n",
    "        best_labels = labels\n",
    "    \n",
    "    print('k =', k, 'has silhouette_score', score)\n",
    "    \n",
    "\n",
    "print(\"Best k = \" + str(best_k))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b. [10 points] For the best choice of k (among 10,20,...,100 that has the highest silhouette score), print 5 *unprocessed* headlines that are closest to each cluster center using cosine distance. There are multiple ways to do this. For example, you could use [pairwise_distances](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise_distances.html#sklearn.metrics.pairwise_distances) (faster). Alternatively you could use [cosine](https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.spatial.distance.cosine.html).\n",
    "\n",
    "Example output:\n",
    "```\n",
    "Cluster 0\n",
    "\tcouncil under fire over water plan\n",
    "\tcouncil work changes plan under fire\n",
    "\tman to face court over northam death\n",
    "\tfeedlot plan divides council\n",
    "\tcouncil urged to rethink water plan support\n",
    "Cluster 1\n",
    "\tlandcare calls for award nominations\n",
    "\tmagical nanny leads olivier award nominations\n",
    "\tnominations sought for regional achiever award\n",
    "\tscottish rockers top award nominations\n",
    "\tchelsea players top award nominations\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you could use either `pairwise_distances` or `cosine` (no need to use both)\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "from scipy.spatial.distance import cosine\n",
    "import numpy as np\n",
    "\n",
    "################################################################################\n",
    "# YOUR CODE GOES HERE\n",
    "################################################################################\n",
    "xdist = pairwise_distances(best_cluster_centers, X)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.argsort(xdist, axis=1)[:, :5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([68983, 58453, 61549, 52475, 63090])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster  0\n",
      "simone cobb\n",
      "footballer regains consciousness\n",
      "noosa triathletes surveyed on satisfaction\n",
      "paralympian conquers kilimanjaro on crutches\n",
      "auctioned tooth has napoleonic roots\n",
      "\n",
      "Cluster  1\n",
      "socceroos finish year ranked 48th\n",
      "uruguayans scared of us socceroos\n",
      "nelsen glad to see back of socceroos\n",
      "emerton agostino to spearhead socceroos\n",
      "rusty moya paradorn move on in chennai\n",
      "\n",
      "Cluster  2\n",
      "diouf steals the show from blackburn\n",
      "ledgers casanova seduces venice\n",
      "vizards mcc successor passionate\n",
      "vermeulen switches to motogp\n",
      "austminex to consolidate mine tenements\n",
      "\n",
      "Cluster  3\n",
      "sir joh taken to hospital\n",
      "sir joh in hospital with pneumonia\n",
      "sir joh comfortable but deteriorating\n",
      "sir joh home from hospital\n",
      "sir joh remains in hospital\n",
      "\n",
      "Cluster  4\n",
      "new detention centres may take asylum seekers\n",
      "asylum seekers leave detention behind\n",
      "planned centres for asylum seekers dropped\n",
      "call for separate detention for asylum seekers\n",
      "baxter protests disturb asylum seekers\n",
      "\n",
      "Cluster  5\n",
      "nrma signs removed as repair dispute escalates\n",
      "ledgers casanova seduces venice\n",
      "sweater too much for sweltering shoplifter\n",
      "acupuncture may relieve heartburn symptoms\n",
      "paracetamol effective for treating osteoarthritis\n",
      "\n",
      "Cluster  6\n",
      "meteorite arouses cambodians superstitions\n",
      "abortions top 84000\n",
      "orioles down firebirds\n",
      "espanyol go third\n",
      "meatworkers stood down at yanco\n",
      "\n",
      "Cluster  7\n",
      "tough times ahead for big banks\n",
      "coles nudges ahead in tough retail times\n",
      "tough times ahead for tyre giant despite profit\n",
      "kerrie lush\n",
      "ghosts exorcised from hamlets castle\n",
      "\n",
      "Cluster  8\n",
      "marauding camels destroy taps toilets\n",
      "first pictures from leupung\n",
      "tonis treble punishes parma\n",
      "kerrie lush\n",
      "scorsese scales down\n",
      "\n",
      "Cluster  9\n",
      "govt to review cityrail officer powers\n",
      "sermons to be scrutinised\n",
      "sexy scent could conquer cockroaches\n",
      "behind the hillsong phenomenon\n",
      "stats expose serie a cheats\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(\"Cluster \", i)\n",
    "    for j in range(5):\n",
    "        print(deduplicated_headlines[indices[i][j]])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Plot the number of headlines in each cluster over time [40 points]\n",
    "\n",
    "Construct a matrix C, where C[i][j] is the number of headlines assigned to cluster j that were published during month i.\n",
    "\n",
    "To help you out, we first provide you the following code that actually determines, for every date in your deduplicated publish date list what its corresponding month is, as an integer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# *** DO NOT MODIFY CODE IN THIS CELL ***\n",
    "# As it turns out, Python has a convenient object for dealing with dates.\n",
    "from datetime import datetime\n",
    "datetime_objects = [datetime.strptime(str(date), '%Y%m%d') for date in deduplicated_publish_dates]\n",
    "\n",
    "# now compute out all the months for each deduplicated date!\n",
    "months = np.array([datetime_object.month for datetime_object in datetime_objects])\n",
    "print(len(months))  # should be 72915\n",
    "print(months)  # should go from 1's to 12's (corresponding to headlines published in January through December)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a. [25 points] Construct `C`:\n",
    "\n",
    "   1. The variable `best_labels` contains the cluster index that each headline is assigned to.\n",
    "   2. The variable `months` contains the month that each headline was published in.\n",
    "   3. For each month `i = 1, ..., 12` and cluster index `j = 0, ..., best_k-1`:\n",
    "      - Count the number of headlines assigned to cluster `j` that were published in month `i`.\n",
    "      - Store this count in `C[i-1][j]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b. [10 points] Given `C`, for each cluster index `0, ..., best_k-1`:\n",
    "\n",
    "   1. Line-plot the number of headlines assigned to that cluster in each month.\n",
    "   2. Ignore cluster indices for which the maximum number of headlines in any month is < 100 (no line-plots for these clusters).\n",
    "   3. There should be one plot for each unignored cluster index. An example plot is below:\n",
    "\n",
    "<img src=\"example.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c. [5 points] Using the headlines corresponding to the cluster centers that you printed in Q3, interpret each of the line plots created above. There is no right answer here, do your best. Example interpretations are:\n",
    "\n",
    "   * Cluster 0: Road accidents peaked in November.\n",
    "   * Cluster 1: Government subsidies were handed out every alternate month.\n",
    "   * Cluster 2: Cannot be interpreted."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
